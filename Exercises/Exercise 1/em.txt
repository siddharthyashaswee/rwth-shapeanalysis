Each cluster is represented by a Gaussian distribution. Thus, our dictionary for each cluster has slightly different keywords to k-means. 
The 2D mean vectors are stored in mean as before. In addition we have a 2x2 covariance matrix stored in cov and a scalar weight w. 


self.cluster
[{'mean': array([-3.80136342,  1.83396855]), 
		'cov': array([[1., 0.],
       [0., 1.]]), 
	   'w': 0.3333333333333333, 
	   'indices': []}, 
	   
	   {'mean': array([ 2.31418896, -2.52312569]), 'cov': array([[1., 0.], [0., 1.]]), 'w': 0.3333333333333333, 'indices': []}, 
	   {'mean': array([-2.55390925, -0.18424666]), 'cov': array([[1., 0.], [0., 1.]]), 'w': 0.3333333333333333, 'indices': []}]


As before the hard assignement of points to clusters is given with indices. The soft assignment from the GMMs can be turned into a hard one by calling assign_points. 

Here every point is assigned to the cluster with the highest assignment score. 

Similar to k-means run alternates between calling the expectation and maximization functions until either the maximum number of iterations is reached or the log likelihood has not changed by a certain amount. 

The soft assignments soft_assignment (ùõæ in the lecture) are represented as a matrix where each row corresponds to each data point and each column corresponds to a Gaussian. The number of soft assigned points is given as a vector in soft_n, where soft_n[i] corresponds to the i-th Gaussian.

In expectation you will have to update soft_assignment and soft_n. To this end we have implemented the bivariate Gaussian probability density function for your convenience in bivariate_gaussian_pdf. 

In maximization you have to update the w, mean, and cov parameters for each Gaussian/cluster.

You can plot the current state with plot_clusters. The means are shown as crosses and the Gaussian distribution as ellipses. If you have called assign_points beforehand then the points will be visualized in different colours depending on their cluster membership.